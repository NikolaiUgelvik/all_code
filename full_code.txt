Directory Tree:
<<<<<<< HEAD
clay/
│   ├── .DS_Store
│   ├── Makefile
│   ├── .devcontainer
│   ├── README.md
│   ├── .gitignore
│   ├── .env
│   ├── docker-compose.yml
│   ├── .env.example
│   ├── entrypoint.sh
│   ├── docker/
│   │   ├── Dockerfile
│   ├── tests/
│   │   ├── test_persistence.py
│   │   ├── test_ingestion.py
│   │   ├── test_transformation.py
│   ├── db/
│   │   ├── init.sql/
│   ├── .git/ [EXCLUDED]
│   ├── .vscode/
│   │   ├── settings.json
│   ├── src/
│   │   ├── .DS_Store
│   │   ├── requirements.txt
│   │   ├── __init__.py
│   │   ├── app.py
│   │   ├── ingestion/
│   │   │   ├── ingestion_agent.py
│   │   │   ├── web_scraper.py
│   │   │   ├── __init__.py
│   │   │   ├── snowflake_ingestion.py
│   │   │   ├── local_ingestion.py
│   │   ├── config/
│   │   │   ├── __init__.py
│   │   │   ├── settings.py
│   │   │   ├── db_init.py
│   │   │   ├── db_base.py
│   │   ├── transformation/
│   │   │   ├── schema_definitions.py
│   │   │   ├── transformations.py
│   │   │   ├── __init__.py
│   │   │   ├── transform_agent.py
│   │   ├── persistence/
│   │   │   ├── __init__.py
│   │   │   ├── data_lake.py
│   │   │   ├── postgres_writer.py
│   │   │   ├── application/
│   │   │   │   ├── __init__.py
│   │   │   ├── infrastructure/
│   │   │   │   ├── __init__.py
│   │   │   ├── domain/
│   │   │   │   ├── measure.py
│   │   │   │   ├── dataentry.py
│   │   │   │   ├── __init__.py
│   │   │   │   ├── dataset.py
│   │   │   │   ├── record.py
│   │   │   │   ├── source.py
│   │   │   │   ├── dimension.py
│   │   │   │   ├── timedimension.py
│   │   ├── api/
│   │   │   ├── __init__.py
│   │   │   ├── main.py
=======
all_code/
│   ├── README.md
│   ├── pyproject.toml
│   ├── uv.lock
│   ├── full_code.txt
│   ├── .gitignore
│   ├── .git/ [EXCLUDED]
│   ├── .venv/ [EXCLUDED]
│   ├── src/
│   │   ├── all_code/
│   │   │   ├── cli.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/ [EXCLUDED]
>>>>>>> c99f4db69f522231f9c8ce75b85e4e5c93b78449




# ======================
<<<<<<< HEAD
# File: docker-compose.yml
# ======================

services:
  db:
    image: postgres:14
    container_name: postgres_db
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql # Optional initialization
    networks:
      - app-network

  app:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: clay
    restart: unless-stopped
    env_file:
      - .env
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
    ports:
      - "8000:8000"
    depends_on:
      - db
    networks:
      - app-network
    volumes:
      - ./src:/code/src
      - ./entrypoint.sh:/code/entrypoint.sh

networks:
  app-network:
    driver: bridge

volumes:
  db_data:


# ======================
# File: entrypoint.sh
# ======================

#!/bin/sh

# Exit immediately if a command exits with a non-zero status
set -e

# Function to check if PostgreSQL is up
wait_for_postgres() {
  echo "Waiting for PostgreSQL to be available..."
  while ! nc -z db 5432; do
    sleep 0.1
  done
  echo "PostgreSQL is up!"
}

# Set PYTHONPATH to include /code
export PYTHONPATH=/code/src

# Wait for PostgreSQL
wait_for_postgres

# Initialize the database (if needed)
echo "Initializing the database..."
python -m src.config.db_init

# Start the main application
echo "Starting the application..."
exec "$@"


# ======================
# File: tests/test_persistence.py
# ======================

"""
Tests for persistence components.
"""

def test_persistence():
    assert True, "Placeholder test for persistence."



# ======================
# File: tests/test_ingestion.py
# ======================

"""
Tests for ingestion components.
"""

def test_ingestion():
    assert True, "Placeholder test for ingestion."



# ======================
# File: tests/test_transformation.py
# ======================

"""
Tests for transformation components.
"""

def test_transformation():
    assert True, "Placeholder test for transformation."



# ======================
# File: src/__init__.py
# ======================

# src/__init__.py
# Initialization for the src package.



# ======================
# File: src/app.py
# ======================

# ======================
# File: src/app.py
# ======================
import time
from api.main import init_api
# ADDED:
from ingestion.ingestion_agent import run_ingestion  # CHANGED (new import)

def main():
    print("Welcome to Clay: Capture, Load, Analyze, and Yield!")
    print("Clay is now running. Press Ctrl+C to stop.")
    init_api()

    # ADDED (Demo usage - optional):
    # If you want a quick demonstration of ingestion from a URL,
    # you can uncomment these lines or handle it differently:
    #
    # url_to_scrape = "https://example.com/some-page-with-csv"
    # run_ingestion(url_to_scrape)

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nShutting down Clay...")


if __name__ == '__main__':
    main()


# ======================
# File: src/ingestion/ingestion_agent.py
# ======================

import os
import requests
import pandas as pd
from pathlib import Path
from typing import List, Dict
from config.db_base import SessionLocal
from sqlalchemy.orm import Session

# Domain imports
from persistence.domain.source import Source
from persistence.domain.dataset import DataSet
from persistence.domain.measure import Measure
from persistence.domain.dimension import Dimension
from persistence.domain.timedimension import TimeDimension
from persistence.domain.record import Record
from persistence.domain.dataentry import DataEntry

# We'll import from web_scraper for scraping logic
from .web_scraper import find_tabular_files

# ADDED (for local file reading)
from .local_ingestion import read_tabular_file  # CHANGED (function we’ll define here or in local_ingestion)

def prompt_user_selection(file_links: List[str]) -> List[str]:
    """
    # ADDED (new function)
    Presents the user with a list of discovered files and asks which ones to ingest.
    Returns a list of the user-selected files.
    """
    if not file_links:
        print("No tabular files found.")
        return []

    print("The following files were found:")
    for i, link in enumerate(file_links, start=1):
        print(f"{i}. {link}")

    selection = input("Enter the file numbers you want to ingest (comma-separated): ")
    if not selection.strip():
        return []
    selected_indices = [int(x.strip()) for x in selection.split(",") if x.strip().isdigit()]

    selected_files = [file_links[i-1] for i in selected_indices if 0 < i <= len(file_links)]
    return selected_files


def download_file(url: str, download_path: str) -> str:
    """
    # ADDED (new function)
    Downloads the file from `url` to `download_path`, returns the local file path.
    """
    local_filename = os.path.join(download_path, os.path.basename(url))
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return local_filename


def infer_schema(df: pd.DataFrame) -> Dict[str, str]:
    """
    # ADDED (new function)
    Infer whether each column is a measure (numeric),
    dimension (string/categorical), or time dimension (date-like).
    Returns a dict of {column_name: "measure"|"dimension"|"time"}
    """
    inferred = {}
    for col in df.columns:
        # Simple heuristic approach
        # Try to see if all non-NA values can parse as float => measure
        # Or parse as date => time
        # else => dimension
        col_data = df[col].dropna()
        if col_data.empty:
            # default to dimension if no data
            inferred[col] = "dimension"
            continue

        # Check date parse
        try:
            pd.to_datetime(col_data, infer_datetime_format=True)
            # If no error => it's a time dimension
            inferred[col] = "time"
            continue
        except:
            pass

        # Check numeric parse
        try:
            col_data.astype(float)
            # If no error => measure
            inferred[col] = "measure"
            continue
        except:
            pass

        # Otherwise dimension
        inferred[col] = "dimension"

    return inferred


def process_dataframe_for_standard_model(
    df: pd.DataFrame,
    inferred_schema: Dict[str, str],
    source_name: str
):
    """
    # ADDED (new function)
    1. Create or retrieve a Source row for source_name.
    2. Create a new DataSet referencing that Source.
    3. Create Measure/Dimension/TimeDimension rows for each column.
    4. For each row, create a Record + DataEntry linking the correct measure/dim/dates.
    """
    session: Session = SessionLocal()

    try:
        # 1. Get or create Source
        source = session.query(Source).filter(Source.name == source_name).first()
        if not source:
            source = Source(name=source_name, description=f"Ingested from {source_name}")
            session.add(source)
            session.commit()

        # 2. Create new DataSet
        dataset = DataSet(source_id=source.id, query={})
        session.add(dataset)
        session.commit()

        # 3. Create measure/dimension/time records
        measure_dict = {}
        dimension_dict = {}
        time_dimension_dict = {}

        for col_name, col_role in inferred_schema.items():
            if col_role == "measure":
                m = Measure(dataset_id=dataset.id, name=col_name)
                session.add(m)
                session.flush()  # get id
                measure_dict[col_name] = m
            elif col_role == "dimension":
                d = Dimension(dataset_id=dataset.id, name=col_name)
                session.add(d)
                session.flush()
                dimension_dict[col_name] = d
            elif col_role == "time":
                t = TimeDimension(dataset_id=dataset.id, name=col_name)
                session.add(t)
                session.flush()
                time_dimension_dict[col_name] = t

        session.commit()

        # 4. For each row in the DataFrame
        for idx, row in df.iterrows():
            record = Record(dataset_id=dataset.id)
            session.add(record)
            session.flush()  # get record.id

            for col_name, value in row.items():
                if pd.isna(value):
                    continue

                if col_name in measure_dict:
                    data_entry = DataEntry(
                        record_id=record.id,
                        measure_id=measure_dict[col_name].id,
                        value=str(value)
                    )
                elif col_name in dimension_dict:
                    data_entry = DataEntry(
                        record_id=record.id,
                        dimension_id=dimension_dict[col_name].id,
                        value=str(value)
                    )
                elif col_name in time_dimension_dict:
                    data_entry = DataEntry(
                        record_id=record.id,
                        time_dimension_id=time_dimension_dict[col_name].id,
                        value=str(value)
                    )
                else:
                    # If it's unclassified for some reason
                    continue

                session.add(data_entry)

            session.commit()

        print(f"Ingested DataFrame into Source(ID={source.id}), DataSet(ID={dataset.id}).")
    except Exception as e:
        session.rollback()
        print(f"Error during ingestion: {e}")
    finally:
        session.close()


def run_ingestion(url: str):
    """
    # CHANGED (entire function)
    Main function to coordinate ingestion from a webpage containing CSV/Excel files.
    """
    # 1. Find tabular files on the page
    results = find_tabular_files(url, file_types=[".csv",".xlsx",".xls",".tsv"])
    file_links = results.get("file_links", [])

    # 2. Prompt user
    selected_files = prompt_user_selection(file_links)
    if not selected_files:
        print("No files selected. Exiting.")
        return

    # 3. Download & read each file, then store
    download_dir = Path("./data_lake")
    download_dir.mkdir(exist_ok=True)

    for link in selected_files:
        local_path = download_file(link, str(download_dir))
        df = read_tabular_file(local_path)  # read into DataFrame
        column_roles = infer_schema(df)
        process_dataframe_for_standard_model(df, column_roles, source_name=url)

    print("Ingestion complete!")

if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage: python ingestion_agent.py <url>")
        sys.exit(1)

    url_to_scrape = sys.argv[1]
    run_ingestion(url_to_scrape)

# ======================
# File: src/ingestion/web_scraper.py
# ======================

import argparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urljoin


def find_tabular_files(url, file_types=None):
    """
    # CHANGED: Renamed from find_downloadable_files to find_tabular_files;
    # extended to handle .tsv if you want, plus any other logic you like.
    Searches a webpage for links to downloadable files that match typical tabular extensions.

    :param url: The website URL to scrape.
    :param file_types: A list of file extensions (e.g., ['.csv', '.xlsx']).
    :return: A dict with "file_links" and optionally "html_tables".
    """

    # Default file extensions if none are provided
    if file_types is None:
        file_types = ['.pdf', '.csv', '.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx',
                      '.zip', '.tar', '.gz', '.rar']

    chrome_options = Options()
    chrome_options.binary_location = "/usr/bin/chromium"
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # CHANGED: Use a fixed path to chromedriver (instead of ChromeDriverManager)
    service = Service("/usr/bin/chromedriver")
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        print(f"Loading page: {url}")
        driver.get(url)

        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "a")))

        links = driver.find_elements(By.TAG_NAME, "a")

        file_links = []
        for link in links:
            href = link.get_attribute("href")
            if href and any(href.lower().endswith(ext) for ext in file_types):
                absolute_url = urljoin(url, href)
                file_links.append(absolute_url)

        return {
            "file_links": file_links,
        }

    except Exception as e:
        print(f"Error: {e}")
        return {"file_links": []}
    finally:
        driver.quit()


def main():
    """
    Command-line interface for scraping downloadable files from a website.
    """
    parser = argparse.ArgumentParser(description="Find downloadable files on a website.")
    parser.add_argument("url", type=str, help="Website URL to scrape")
    parser.add_argument(
        "--file-types",
        type=str,
        default=".pdf,.csv,.xls,.xlsx,.doc,.docx,.ppt,.pptx,.zip,.tar,.gz,.rar",
        help="Comma-separated list of file extensions to filter (e.g., .pdf,.csv)"
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        default=True,
        help="Run in headless mode (default: True)"
    )

    args = parser.parse_args()
    file_types = args.file_types.split(",")

    result = find_tabular_files(args.url, file_types)

    files = result.get("file_links", [])
    if files:
        print("\nDownloadable files found:")
        for file in files:
            print(file)
    else:
        print("\nNo downloadable files found.")


if __name__ == "__main__":
    main()


# ======================
# File: src/ingestion/__init__.py
# ======================

# src/ingestion/__init__.py
# Initialization for ingestion module.



# ======================
# File: src/ingestion/snowflake_ingestion.py
# ======================

"""
Logic for connecting to Snowflake tables/views and fetching data.
"""

def ingest_from_snowflake(table_name: str):
    """
    Query a Snowflake table/view and return data as a DataFrame.
    """
    pass



# ======================
# File: src/ingestion/local_ingestion.py
# ======================

# ======================
# File: src/ingestion/local_ingestion.py
# ======================
"""
Logic for reading files from a local directory or local paths.
"""

import os
import pandas as pd

def ingest_from_local(directory_path: str):
    """
    # No changes from the earlier placeholder if you want to keep a scanning approach.
    # Could be used to find local CSV/XLS files, read them, and pass to processing.
    """
    pass

def read_tabular_file(local_file_path: str) -> pd.DataFrame:
    """
    # ADDED (new function) - used by ingestion_agent
    Reads CSV or Excel from a local path into a DataFrame.
    Auto-detect extension and read accordingly.
    """
    ext = os.path.splitext(local_file_path)[1].lower()
    if ext in [".csv", ".txt", ".tsv"]:
        df = pd.read_csv(local_file_path)
    elif ext in [".xls", ".xlsx"]:
        df = pd.read_excel(local_file_path)
    else:
        raise ValueError(f"Unrecognized file extension: {ext}")
    return df


# ======================
# File: src/config/__init__.py
# ======================



# ======================
# File: src/config/settings.py
# ======================

"""
Global settings, environment variables, logging configurations, etc.
"""

import os

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'YOUR_DEFAULT_API_KEY')
SNOWFLAKE_USER = os.environ.get('SNOWFLAKE_USER', '...')
# Add more environment-based settings as needed.



# ======================
# File: src/config/db_init.py
# ======================

import logging
from sqlalchemy import create_engine, text
import os
from config.db_base import Base, engine  # updated import statement

# Import all models to ensure they are registered with Base
from persistence.domain.dataentry import DataEntry
from persistence.domain.dataset import DataSet
from persistence.domain.dimension import Dimension
from persistence.domain.measure import Measure
from persistence.domain.record import Record
from persistence.domain.source import Source
from persistence.domain.timedimension import TimeDimension

# Enable SQLAlchemy logging
logging.basicConfig()
logging.getLogger("sqlalchemy.engine").setLevel(logging.INFO)

DATABASE_URL = os.getenv("DATABASE_URL")

if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")


def ensure_database_exists():
    """Check if the database exists and create it if not."""
    db_name = os.getenv("POSTGRES_DB")
    postgres_user = os.getenv("POSTGRES_USER")
    postgres_password = os.getenv("POSTGRES_PASSWORD")
    postgres_host = "db"  # Use Docker service name as the hostname
    postgres_port = "5432"

    # Base URL to connect to the `postgres` system database
    base_url = (
        f"postgresql://{postgres_user}:{postgres_password}@"
        f"{postgres_host}:{postgres_port}/postgres"
    )

    # Connect to the system database
    temp_engine = create_engine(base_url, isolation_level="AUTOCOMMIT")

    print(f"Checking if database '{db_name}' exists...")
    with temp_engine.connect() as conn:
        # Query to check if the database exists
        result = conn.execute(
            text("SELECT 1 FROM pg_database WHERE datname = :db_name"),
            {"db_name": db_name}
        ).scalar()

        # Create the database if it does not exist
        if not result:
            print(f"Database '{db_name}' does not exist. Creating...")
            conn.execute(text(f"CREATE DATABASE {db_name}"))
            print(f"Database '{db_name}' created successfully.")
        else:
            print(f"Database '{db_name}' already exists.")


ensure_database_exists()


def init_db():
    print("Creating database tables...")

    # Debugging: Print the tables in Base.metadata
    print("Tables in Base.metadata:")
    for table_name in Base.metadata.tables.keys():
        print(table_name)

    # Create all tables defined in the Base's metadata
    Base.metadata.create_all(bind=engine)
    print("Database tables created successfully.")


if __name__ == "__main__":
    init_db()


# ======================
# File: src/config/db_base.py
# ======================

from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker
import os

DATABASE_URL = os.getenv("DATABASE_URL")

if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")

engine = create_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


# ======================
# File: src/transformation/schema_definitions.py
# ======================

"""
Common schema definitions for standardizing data fields.
"""

COMMON_SCHEMA = {
    # Example: 'full_name': 'string', 'city': 'string', etc.
}



# ======================
# File: src/transformation/transformations.py
# ======================

"""
Custom transformation utility functions (e.g., date parsing, column renaming).
"""

def unify_schema(df, target_schema):
    """
    Apply transformations to match df columns to a target schema.
    """
    pass



# ======================
# File: src/transformation/__init__.py
# ======================

# src/transformation/__init__.py
# Initialization for transformation module.



# ======================
# File: src/transformation/transform_agent.py
# ======================

"""
LangChain agent for schema normalization and data transformation.
"""

def run_transform(df):
    """
    Analyze a DataFrame, create or apply transformations
    to fit a common schema.
    """
    pass



# ======================
# File: src/persistence/__init__.py
# ======================

# src/persistence/__init__.py
# Initialization for persistence module.



# ======================
# File: src/persistence/data_lake.py
# ======================

"""
Utilities for storing data locally in a 'data lake' format (e.g. Parquet).
"""

def save_as_parquet(df, path):
    """
    Save the given DataFrame as a Parquet file at the specified path.
    """
    pass



# ======================
# File: src/persistence/postgres_writer.py
# ======================

"""
Functions for writing DataFrames to a Docker-hosted Postgres database.
"""

def write_to_postgres(df, db_code, table_name):
    """
    Write the DataFrame to a Postgres table. The 'db_code' can identify
    which database or schema to target.
    """
    pass



# ======================
# File: src/persistence/application/__init__.py
# ======================



# ======================
# File: src/persistence/infrastructure/__init__.py
# ======================



# ======================
# File: src/persistence/domain/measure.py
# ======================

from sqlalchemy import Column, Integer, String, Text, ForeignKey
from sqlalchemy.orm import relationship
from config.db_base import Base
from .dataset import DataSet 

class Measure(Base):
    __tablename__ = 'measures'

    id = Column(Integer, primary_key=True, index=True)
    dataset_id = Column(Integer, ForeignKey('datasets.id'), nullable=False)
    name = Column(String(255), nullable=False)
    title = Column(String(255), nullable=True)
    short_title = Column(String(100), nullable=True)
    description = Column(Text, nullable=True)
    type = Column(String(100), nullable=True)

    dataset = relationship("DataSet", back_populates="measures")
    data_entries = relationship("DataEntry", back_populates="measure")

    def __repr__(self):
        return f"<Measure(id={self.id}, name='{self.name}', dataset_id={self.dataset_id})>"


# ======================
# File: src/persistence/domain/dataentry.py
# ======================

from sqlalchemy import Column, Integer, String, ForeignKey
from sqlalchemy.orm import relationship
from config.db_base import Base  # Import Base from db_base
from .dataset import DataSet  # Ensure DataSet is imported


class DataEntry(Base):
    __tablename__ = 'data_entries'

    id = Column(Integer, primary_key=True, index=True)
    record_id = Column(Integer, ForeignKey('records.id'), nullable=False)
    measure_id = Column(Integer, ForeignKey('measures.id'), nullable=True)
    dimension_id = Column(Integer, ForeignKey('dimensions.id'), nullable=True)
    time_dimension_id = Column(Integer, ForeignKey(
        'time_dimensions.id'), nullable=True)
    value = Column(String(255), nullable=False)

    record = relationship("Record", back_populates="data_entries")
    measure = relationship("Measure", back_populates="data_entries")
    dimension = relationship("Dimension", back_populates="data_entries")
    time_dimension = relationship(
        "TimeDimension", back_populates="data_entries")

    def __repr__(self):
        return f"<DataEntry(id={self.id}, value='{self.value}', record_id={self.record_id})>"


# ======================
# File: src/persistence/domain/__init__.py
# ======================

from .dataentry import DataEntry
from .dataset import DataSet
from .dimension import Dimension
from .measure import Measure
from .record import Record
from .source import Source
from .timedimension import TimeDimension

__all__ = [
    "DataEntry",
    "DataSet",
    "Dimension",
    "Measure",
    "Record",
    "Source",
    "TimeDimension"
]


# ======================
# File: src/persistence/domain/dataset.py
# ======================

from sqlalchemy import Column, Integer, ForeignKey, DateTime
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship
from datetime import datetime
from config.db_base import Base


class DataSet(Base):
    __tablename__ = 'datasets'

    id = Column(Integer, primary_key=True, index=True)
    source_id = Column(Integer, ForeignKey('sources.id'))
    query = Column(JSONB, nullable=False)
    annotations = Column(JSONB, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    source = relationship("Source", back_populates="datasets")
    measures = relationship(
        "Measure", back_populates="dataset", cascade="all, delete-orphan")
    dimensions = relationship(
        "Dimension", back_populates="dataset", cascade="all, delete-orphan")
    time_dimensions = relationship(
        "TimeDimension", back_populates="dataset", cascade="all, delete-orphan")
    records = relationship(
        "Record", back_populates="dataset", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<DataSet(id={self.id}, source_id={self.source_id})>"


# ======================
# File: src/persistence/domain/record.py
# ======================

from sqlalchemy import Column, Integer, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from datetime import datetime
from config.db_base import Base
from .dataset import DataSet  # Ensure DataSet is imported

class Record(Base):
    __tablename__ = 'records'

    id = Column(Integer, primary_key=True, index=True)
    dataset_id = Column(Integer, ForeignKey('datasets.id'), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    dataset = relationship("DataSet", back_populates="records")
    data_entries = relationship(
        "DataEntry", back_populates="record", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Record(id={self.id}, dataset_id={self.dataset_id}, created_at={self.created_at})>"


# ======================
# File: src/persistence/domain/source.py
# ======================

from sqlalchemy import Column, Integer, String, Text
from sqlalchemy.orm import relationship
from config.db_base import Base
from .dataset import DataSet  # Ensure DataSet is imported


class Source(Base):
    __tablename__ = 'sources'

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(255), nullable=False, unique=True)
    description = Column(Text, nullable=True)

    datasets = relationship("DataSet", back_populates="source")

    def __repr__(self):
        return f"<Source(id={self.id}, name='{self.name}')>"


# ======================
# File: src/persistence/domain/dimension.py
# ======================

from sqlalchemy import Column, Integer, String, ForeignKey
from sqlalchemy.orm import relationship
from config.db_base import Base


class Dimension(Base):
    __tablename__ = 'dimensions'

    id = Column(Integer, primary_key=True, index=True)
    dataset_id = Column(Integer, ForeignKey('datasets.id'), nullable=False)
    name = Column(String(255), nullable=False)
    title = Column(String(255), nullable=True)
    short_title = Column(String(100), nullable=True)
    type = Column(String(100), nullable=True)

    dataset = relationship("DataSet", back_populates="dimensions")
    data_entries = relationship("DataEntry", back_populates="dimension")

    def __repr__(self):
        return f"<Dimension(id={self.id}, name='{self.name}', dataset_id={self.dataset_id})>"


# ======================
# File: src/persistence/domain/timedimension.py
# ======================

from sqlalchemy import Column, Integer, String, Text, ForeignKey
from sqlalchemy.orm import relationship
from config.db_base import Base
from .dataset import DataSet  # Ensure DataSet is imported


class TimeDimension(Base):
    __tablename__ = 'time_dimensions'

    id = Column(Integer, primary_key=True, index=True)
    dataset_id = Column(Integer, ForeignKey('datasets.id'), nullable=False)
    name = Column(String(255), nullable=False)
    title = Column(String(255), nullable=True)
    short_title = Column(String(100), nullable=True)
    description = Column(Text, nullable=True)
    type = Column(String(100), nullable=True)
    granularity = Column(String(100), nullable=True)

    dataset = relationship("DataSet", back_populates="time_dimensions")
    data_entries = relationship("DataEntry", back_populates="time_dimension")

    def __repr__(self):
        return f"<TimeDimension(id={self.id}, name='{self.name}', granularity='{self.granularity}', dataset_id={self.dataset_id})>"


# ======================
# File: src/api/__init__.py
# ======================



# ======================
# File: src/api/main.py
# ======================

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Welcome to Clay: Capture, Load, Analyze, and Yield!"}

@app.get("/status")
def get_status():
    return {"status": "Clay is running"}

def init_api():
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
=======
# File: src/all_code/cli.py
# ======================

# Copyright 2024 Spencer Bentley

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”),
# to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
# IN THE SOFTWARE.

import os
from dataclasses import dataclass
from typing import Set

import typer
from typing_extensions import Annotated

# Initialize Typer app
app = typer.Typer()

# Define the master file name
FULL_CODE_FILE_NAME: str = "full_code.txt"

FILES_TO_INCLUDE: Set[str] = set()  # if empty, include all files

# Define programming-related file extensions (removed '.json' and '.md')
PROGRAMMING_EXTENSIONS: Set[str] = {
    ".py",
    ".java",
    ".c",
    ".cpp",
    ".js",
    ".ts",
    ".html",
    ".css",
    ".rb",
    ".go",
    ".php",
    ".swift",
    ".kt",
    ".rs",
    ".scala",
    ".sh",
    ".pl",
    ".lua",
    ".sql",
    ".xml",
    ".yml",
    ".yaml",
    ".json",
}

# Define directories to exclude during file aggregation and directory tree generation
EXCLUDE_DIRS: Set[str] = {
    "venv",
    ".venv",
    "node_modules",
    "__pycache__",
    ".git",
    "dist",
    "build",
}

# Define files to exclude from both aggregation and directory tree
EXCLUDE_FILES: Set[str] = {"package-lock.json", "package.json"}


@dataclass
class Config:
    full_code_file_name: str
    files_to_include: Set[str]
    programming_extensions: Set[str]
    exclude_dirs: Set[str]


def generate_directory_tree(startpath: str, config: Config) -> str:
    """
    Generates an ASCII directory tree.
    - Excluded directories and their subdirectories are only listed by name without their internal files.
    - The script itself is excluded from the tree.

    Args:
        startpath (str): The root directory to start generating the tree from.
        config (Config): Configuration object containing settings.

    Returns:
        str: The generated directory tree as a string.
    """
    tree = ""
    for root, dirs, files in os.walk(startpath):
        # Determine the relative path from the startpath
        rel_path = os.path.relpath(root, startpath)
        if rel_path == ".":
            rel_path = ""

        # Split the relative path into parts
        path_parts = rel_path.split(os.sep) if rel_path else []

        # Calculate the level of depth
        level = len(path_parts)
        indent = "│   " * level + "├── " if level > 0 else ""

        # Add the current directory to the tree
        current_dir = (
            os.path.basename(root)
            if rel_path
            else os.path.basename(startpath.rstrip(os.sep)) or startpath
        )

        # Check if the current directory is excluded
        if current_dir in config.exclude_dirs:
            tree += f"{indent}{current_dir}/ [EXCLUDED]\n"
            dirs[:] = []  # Prevent os.walk from traversing further
            continue

        tree += f"{indent}{current_dir}/\n"

        # List files, excluding the script itself and any in EXCLUDE_FILES
        for f in files:
            if f in EXCLUDE_FILES:
                continue  # Skip excluded files
            tree += f"{'│   ' * (level + 1)}├── {f}\n"

    return tree


def is_programming_file(filename: str, config: Config) -> bool:
    """
    Checks if the file has a programming-related extension.

    Args:
        filename (str): The name of the file.
        config (Config): Configuration object containing settings.

    Returns:
        bool: True if the file is a programming file, False otherwise.
    """
    _, ext = os.path.splitext(filename)
    return ext.lower() in config.programming_extensions


def should_exclude(path: str, config: Config) -> bool:
    """
    Determines if a file should be excluded based on its path.
    - Excludes files in EXCLUDE_DIRS and their subdirectories.
    - Excludes files listed in EXCLUDE_FILES.

    Args:
        path (str): The file path.
        config (Config): Configuration object containing settings.

    Returns:
        bool: True if the file should be excluded, False otherwise.
    """
    # Normalize path separators
    normalized_path = os.path.normpath(path)
    parts = normalized_path.split(os.sep)

    # Check if any part of the path is in the EXCLUDE_DIRS
    for part in parts[:-1]:  # Exclude the file name itself
        if part in config.exclude_dirs:
            return True

    # Check if the file itself is in EXCLUDE_FILES
    if parts[-1] in EXCLUDE_FILES:
        return True

    return False


def should_include_file(file_path: str, config: Config) -> bool:
    """
    Determines if a file should be included based on FILES_TO_INCLUDE.
    If FILES_TO_INCLUDE is empty, include all files.

    Args:
        file_path (str): The path to the file.
        config (Config): Configuration object containing settings.

    Returns:
        bool: True if the file should be included, False otherwise.
    """
    if not config.files_to_include:
        return True  # Include all files if the list is empty
    rel_file_path = os.path.relpath(file_path)
    return rel_file_path in config.files_to_include


@app.command()
def generate_full_code(
    full_code_file_name: Annotated[
        str,
        typer.Option(
            "--output-file",
            "-o",
            help="Name of the master file to create.",
            show_default=True,
        ),
    ] = FULL_CODE_FILE_NAME,
    files_to_include: Annotated[
        str,
        typer.Option(
            "--include-files",
            "-i",
            help="Comma-separated list of specific files to include. If not provided, all files are included.",
        ),
    ] = None,
    programming_extensions: Annotated[
        str,
        typer.Option(
            "--extensions",
            "-x",
            help=f"Comma-separated list of programming file extensions to include. Default '{', '.join(PROGRAMMING_EXTENSIONS)}'.",
            show_default=False,
        ),
    ] = None,
    exclude_dirs: Annotated[
        str,
        typer.Option(
            "--exclude-dirs",
            "-e",
            help=f"Comma-separated list of directories to join the default exclude dirs list '{', '.join(EXCLUDE_DIRS)}'",
            show_default=False,
        ),
    ] = None,
):
    """
    Generate a master file containing the directory tree and contents of specified programming files.
    """
    # Process comma-separated input strings into sets
    if files_to_include:
        files_to_include_set = set(map(str.strip, files_to_include.split(",")))
    else:
        files_to_include_set = FILES_TO_INCLUDE

    if programming_extensions:
        programming_extensions_set = set(
            map(str.strip, programming_extensions.split(","))
        )
    else:
        programming_extensions_set = PROGRAMMING_EXTENSIONS

    if exclude_dirs:
        exclude_dirs_set = EXCLUDE_DIRS.union(
            set(map(str.strip, exclude_dirs.split(",")))
        )
    else:
        exclude_dirs_set = EXCLUDE_DIRS

    # Create a Config instance
    config = Config(
        full_code_file_name=full_code_file_name,
        files_to_include=files_to_include_set,
        programming_extensions=programming_extensions_set,
        exclude_dirs=exclude_dirs_set,
    )

    startpath = os.getcwd()

    # Generate directory tree
    directory_tree = generate_directory_tree(startpath, config)

    # Open the master file for writing
    try:
        with open(config.full_code_file_name, "w", encoding="utf-8") as master_file:
            # Write the directory tree at the top
            master_file.write("Directory Tree:\n")
            master_file.write(directory_tree)
            master_file.write("\n\n")

            # Traverse the directory again to process files
            for root, dirs, files in os.walk(startpath):
                # Determine the relative path from the startpath
                rel_path = os.path.relpath(root, startpath)
                if rel_path == ".":
                    rel_path = ""

                # Split the relative path into parts
                path_parts = rel_path.split(os.sep) if rel_path else []

                # Check if any parent directory is excluded
                if any(part in config.exclude_dirs for part in path_parts):
                    # Skip processing this directory and its subdirectories
                    dirs[:] = []  # Prevent os.walk from traversing further
                    continue

                # Check if the current directory is excluded
                current_dir: str = (
                    os.path.basename(root)
                    if rel_path
                    else os.path.basename(startpath.rstrip(os.sep)) or startpath
                )
                if current_dir in config.exclude_dirs:
                    dirs[:] = []  # Prevent os.walk from traversing further
                    continue

                for file in files:
                    if not is_programming_file(file, config):
                        continue  # Skip non-programming files

                    file_path = os.path.join(root, file)
                    # Get relative path for exclusion and headers
                    rel_file_path = os.path.relpath(file_path, startpath)

                    if should_exclude(rel_file_path, config) or not should_include_file(
                        file_path, config
                    ):
                        continue  # Skip excluded files or those not in FILES_TO_INCLUDE

                    header: str = (
                        f"\n\n# ======================\n# File: {rel_file_path}\n# ======================\n\n"
                    )
                    master_file.write(header)

                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()
                            master_file.write(content)
                    except Exception as e:
                        error_msg: str = (
                            f"\n# Error reading file {rel_file_path}: {e}\n"
                        )
                        master_file.write(error_msg)

        typer.echo(
            typer.style(
                f"Full code file '{config.full_code_file_name}' has been created successfully.",
                fg=typer.colors.GREEN,
            )
        )

    except Exception as e:
        typer.echo(
            typer.style(
                f"An error occurred while creating the full code file: {e}",
                fg=typer.colors.RED,
            )
        )


# ======================
# File: src/all_code/__init__.py
# ======================

>>>>>>> c99f4db69f522231f9c8ce75b85e4e5c93b78449
